Command:        mpiexec -n 4 ./wave_c
Resources:      1 node (2 physical, 2 logical cores per node)
Memory:         4 GiB per node
Tasks:          4 processes
Machine:        aposta-VirtualBox
Start time:     周五 8月 11 2017 11:58:09 (UTC+08)
Total time:     31 seconds
Full path:      /home/aposta/sc17/allinea/reports/examples

Summary: wave_c is MPI-bound in this configuration
Compute:                                     37.5% |===|
MPI:                                         62.5% |=====|
I/O:                                          0.0% |
This application run was MPI-bound. A breakdown of this time and advice for investigating further is in the MPI section below. 

CPU:
A breakdown of the 37.5% CPU time:
Scalar numeric ops:                          46.3% |====|
Vector numeric ops:                           0.0% |
Memory accesses:                             53.7% |====|
The per-core performance is memory-bound. Use a profiler to identify time-consuming loops and check their cache performance.
No time is spent in vectorized instructions. Check the compiler's vectorization advice to see why key loops could not be vectorized.

MPI:
A breakdown of the 62.5% MPI time:
Time in collective calls:                     3.4% ||
Time in point-to-point calls:                96.6% |=========|
Effective process collective rate:            2.03 kB/s
Effective process point-to-point rate:         255 kB/s
Most of the time is spent in point-to-point calls with a very low transfer rate. This suggests load imbalance is causing synchronization overhead; use an MPI profiler to investigate.

I/O:
A breakdown of the 0.0% I/O time:
Time in reads:                                0.0% |
Time in writes:                               0.0% |
Effective process read rate:                  0.00 bytes/s
Effective process write rate:                 0.00 bytes/s
No time is spent in I/O operations. There's nothing to optimize here!

Threads:
A breakdown of how multiple threads were used:
Computation:                                  0.0% |
Synchronization:                              0.0% |
Physical core utilization:                  100.0% |=========|
System load:                                202.2% |===================|
No measurable time is spent in multithreaded code.
The system load is high. Check that other jobs or system processes are not running on the same nodes.

Memory:
Per-process memory usage may also affect scaling:
Mean process memory usage:                    30.9 MiB
Peak process memory usage:                    38.8 MiB
Peak node memory usage:                      49.0% |====|
The peak node memory usage is low. Running with fewer MPI processes and more data on each process may be more efficient.

Energy:
A breakdown of how energy was used:
CPU:                                      not supported
System:                                   not supported
Mean node power:                          not supported
Peak node power:                          not supported
Energy metrics are not available on this system.
CPU metrics are not supported (no intel_rapl module)

