Command:        mpirun -np 2 ./xhpl
Resources:      1 node (2 physical, 2 logical cores per node)
Memory:         4 GiB per node
Tasks:          2 processes
Machine:        aposta-VirtualBox
Start time:     周五 8月 11 2017 12:19:19 (UTC+08)
Total time:     99 seconds (about 2 minutes)
Full path:      /home/aposta/sc17/hpl-2.2/bin/openBlas

Summary: xhpl is Compute-bound in this configuration
Compute:                                     58.8% |=====|
MPI:                                         41.2% |===|
I/O:                                      &lt;0.1% ||
This application run was Compute-bound. A breakdown of this time and advice for investigating further is in the CPU section below. 

CPU:
A breakdown of the 58.8% CPU time:
Scalar numeric ops:                           9.1% ||
Vector numeric ops:                          25.4% |==|
Memory accesses:                             44.7% |===|
The per-core performance is memory-bound. Use a profiler to identify time-consuming loops and check their cache performance.

MPI:
A breakdown of the 41.2% MPI time:
Time in collective calls:                     0.1% ||
Time in point-to-point calls:                99.9% |=========|
Effective process collective rate:            0.00 bytes/s
Effective process point-to-point rate:        40.7 MB/s
Most of the time is spent in point-to-point calls with a low transfer rate. This can be caused by inefficient message sizes, such as many small messages, or by imbalanced workloads causing processes to wait.

I/O:
A breakdown of the <0.1% I/O time:
Time in reads:                                0.0% |
Time in writes:                             100.0% |=========|
Effective process read rate:                  0.00 bytes/s
Effective process write rate:                  237 kB/s
Most of the time is spent in write operations with a very low effective transfer rate. This may be caused by contention for the filesystem or inefficient access patterns. Use an I/O profiler to investigate which write calls are affected.

Threads:
A breakdown of how multiple threads were used:
Computation:                                 16.2% |=|
Synchronization:                             83.8% |=======|
Physical core utilization:                   82.7% |=======|
System load:                                195.1% |===================|
Significant time is spent synchronizing threads. Check which locks cause the most overhead with a profiler.
The system load is high. Ensure background system processes are not running.

Memory:
Per-process memory usage may also affect scaling:
Mean process memory usage:                    64.8 MiB
Peak process memory usage:                    73.2 MiB
Peak node memory usage:                      48.0% |====|
The peak node memory usage is low. Running with fewer MPI processes and more data on each process may be more efficient.

Energy:
A breakdown of how energy was used:
CPU:                                      not supported
System:                                   not supported
Mean node power:                          not supported
Peak node power:                          not supported
Energy metrics are not available on this system.
CPU metrics are not supported (no intel_rapl module)

